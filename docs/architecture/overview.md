# Overview

Maigie is an AI-powered student companion that helps learners manage courses, set goals, discover resources, schedule study sessions, get forecasts and reminders, and converse with an intelligent assistant (text + voice). The codebase is a monorepo managed with **Nx**. The frontend stack uses **Vite + shadcn-ui** for web and **Expo** for mobile (React Native). The backend is **FastAPI** with clear service separation and AI integrations (vector DB, embeddings, LLMs).

---

# High-level Requirements & Features

* **AI‑first operations**: Courses, goals, and schedules can be *auto-generated* by the AI assistant during conversations.

  * User can manually edit or add more items, but AI generation is the default.
  * Example: User says *"I want to learn Data Structures for my exam in 6 weeks"* → AI creates a course outline, sets goals, and builds a study schedule.
  * Example: User says *"Help me plan tomorrow"* → AI analyzes workload and generates a daily schedule.

* User auth (email/password, social sign-in)

* Create & manage Courses (AI-generated syllabus or manual input)

* Create Goals (AI interprets intent and sets milestones automatically)

* Resource recommendations (links, videos, PDFs) — generated by AI and curated by user

* Schedule (daily view, time blocks, recurring sessions) — AI suggests optimized plan automatically

* Forecast (AI-driven estimates of progress by date given study habits)

* Reminders & notifications (push, email)

* AI Assistant (chat + voice): ask about courses, get study plans, request resources, ask clarifying questions

* Dashboard organization: courses, goals, resources, daily schedule, forecast, reminders

* Offline-first support on mobile (Expo + local DB like WatermelonDB / SQLite sync)

* Analytics, usage metrics, privacy and consent controls

---

# Non-functional Considerations

* Design for incremental adoption: start with text-chat, notes, courses and a simple scheduler; add voice and forecasting later.
* Keep per-user data partitioned and scalable.
* Cost control for LLM usage: caching, summarization instead of full re-generation for repeated queries, tiered quotas.

